{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(d_model, vocab_size)\n",
    "        # shape = [num of words * dimension of embedding layer]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(d_model)\n",
    "        # dimension same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, seq_length, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(self.seq_length, self.d_model)  # To get the matrix of dimension as of embedding layer\n",
    "        positions = torch.arange(0, self.seq_length, dtype = torch.float32).unsqueeze(1)  # matrix of [seq_length x 1]\n",
    "        div_term = (positions /(torch.pow(10000, 2 * torch.arange(0, d_model, 2).float() /self.d_model))) #to calculate say (angle)  pos/(10000^(2i/dmodel))\n",
    "        pe[:, 0::2] = torch.sin(div_term)   #Apply sine formula in even positions\n",
    "        pe[:, 1::2] = torch.cos(div_term)   # Appply cosine formula in odd positions\n",
    "        \n",
    "        self.pe = pe.unsqueeze(0)  # for batches dimension [1 x seq_length x d_model]\n",
    "\n",
    "        # self.register_buffer('pe', self.pe) # By adding this in register buffer this stores pe too while saving the model without considering it as a learning parameter\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe.required_grad(False)  #To make it not to learn\n",
    "        return self.dropout(x)\n",
    "    # def forward(self, ..):\n",
    "        # pe = torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = nn.Parameter(torch.ones(1))  # Scale\n",
    "        self.beta = nn.Parameter(torch.zeros(1))  # Shift\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, keepdim=True) \n",
    "        return self.gamma * (x - mean) / torch.sqrt(var + self.epsilon) + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dff, dropout = 0.5):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.forward1 = nn.Linear(d_model, dff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.forward2 = nn.Linear(dff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.forward2(self.dropout(torch.relu(self.forward1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE I USED ALL EMBEDDING FOR EACH HEAD AND CONCATENATE THEM AND USE LINEAR TRANSFORMATION TO GET THE OUTPUT SAME DIMENSION AS INPUT\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model, heads, dropout = 0.5):\n",
    "#         super(MultiHeadAttention, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.heads = heads\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         self.w_q = nn.ModuleList(nn.Linear(d_model, d_model) for _ in range(heads))\n",
    "#         self.w_k = nn.ModuleList(nn.Linear(d_model, d_model) for _ in range(heads))\n",
    "#         self.w_v = nn.ModuleList(nn.Linear(d_model, d_model) for _ in range(heads))\n",
    "\n",
    "#         self.w_o = nn.Linear(d_model * heads, d_model)\n",
    "\n",
    "#         self.softmax = nn.Softmax(dim = -1)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, embeded_layer):\n",
    "\n",
    "#         attention_outputs = []\n",
    "\n",
    "#         for head in range(self.heads):\n",
    "        \n",
    "#             query = self.w_q[head](embeded_layer)\n",
    "#             key = self.w_k[head](embeded_layer)\n",
    "#             value = self.w_v[head](embeded_layer)\n",
    "\n",
    "#             similarity = torch.matmul(query, torch.transpose(key, -2, -1))  / math.sqrt(self.d_model)\n",
    "\n",
    "#             sim = self.softmax(similarity)\n",
    "#             sim = self.dropout(sim)\n",
    "\n",
    "#             final = torch.matmul(sim, value)\n",
    "\n",
    "#             attention_outputs.append(final)\n",
    "            \n",
    "#         concat_matrix = torch.cat(attention_outputs, -1)\n",
    "#         print(concat_matrix.shape)\n",
    "#         print(self.w_o.weight.shape)\n",
    "#         return self.w_o(concat_matrix)\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads, dropout = 0.5):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_heads = d_model//heads\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    # def splitweights(self, x):\n",
    "    #     batch_size, seq_len, d_model = x.shape\n",
    "    #     x = x.view(batch_size, seq_len, self.heads, -1)\n",
    "    #     return x.permute(0, 2, 1, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, x_q, x_k, x_v, mask = None):\n",
    "\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        print(x.shape)\n",
    "        print(self.w_q.weight.shape)\n",
    "\n",
    "        query = self.w_q(x_q).view(batch_size, seq_len, self.heads, -1).permute(0, 2, 1, 3)\n",
    "        key = self.w_k(x_k).view(batch_size, seq_len, self.heads, -1).permute(0, 2, 1, 3)\n",
    "        value = self.w_v(x_v).view(batch_size, seq_len, self.heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        # query = self.splitweights(self.w_q(x))\n",
    "        # key = self.splitweights(self.w_k(x))\n",
    "        # value = self.splitweights(self.w_v(x))\n",
    "        print(query.shape)\n",
    "\n",
    "        similarity = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_heads)\n",
    "\n",
    "        # print(similarity.shape)\n",
    "\n",
    "        if mask is not None:\n",
    "            # print(mask)\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "            print(mask)\n",
    "            # print(similarity)\n",
    "            similarity = similarity.masked_fill(mask == 0, float('-inf'))\n",
    "        print(similarity)\n",
    "\n",
    "        sim = self.softmax(similarity)\n",
    "        print(sim)\n",
    "        sim = self.dropout(sim)\n",
    "\n",
    "        # print(sim)\n",
    "\n",
    "        final = torch.matmul(sim, value)\n",
    "\n",
    "        final = final.permute(0, 2, 1, 3).contiguous()\n",
    "        final = final.view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "\n",
    "        # print(final.shape)\n",
    "        return self.w_o(final)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 15])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([2, 3, 4, 5])\n",
      "tensor([[[[1., 0., 0., 0.],\n",
      "          [1., 1., 0., 0.],\n",
      "          [1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1.]]]])\n",
      "tensor([[[[ 0.1717,    -inf,    -inf,    -inf],\n",
      "          [ 0.0528, -0.0770,    -inf,    -inf],\n",
      "          [ 0.2192,  0.0309,  0.2155,    -inf],\n",
      "          [ 0.0830, -0.0040,  0.0589, -0.0043]],\n",
      "\n",
      "         [[-0.1358,    -inf,    -inf,    -inf],\n",
      "          [-0.0590,  0.0006,    -inf,    -inf],\n",
      "          [-0.0116,  0.0284,  0.0125,    -inf],\n",
      "          [-0.1001, -0.0456, -0.0921, -0.0457]],\n",
      "\n",
      "         [[ 0.3085,    -inf,    -inf,    -inf],\n",
      "          [ 0.2544,  0.1582,    -inf,    -inf],\n",
      "          [ 0.3826,  0.2142, -0.0424,    -inf],\n",
      "          [ 0.1670,  0.0294, -0.1889,  0.0341]]],\n",
      "\n",
      "\n",
      "        [[[-0.0757,    -inf,    -inf,    -inf],\n",
      "          [ 0.1538,  0.1260,    -inf,    -inf],\n",
      "          [ 0.0635,  0.0912, -0.0412,    -inf],\n",
      "          [ 0.0837,  0.0596,  0.0208,  0.1291]],\n",
      "\n",
      "         [[-0.0356,    -inf,    -inf,    -inf],\n",
      "          [-0.0256, -0.0612,    -inf,    -inf],\n",
      "          [-0.0018,  0.0120, -0.0327,    -inf],\n",
      "          [-0.0117, -0.0163, -0.0562,  0.1319]],\n",
      "\n",
      "         [[-0.1432,    -inf,    -inf,    -inf],\n",
      "          [-0.1258, -0.1070,    -inf,    -inf],\n",
      "          [-0.0612, -0.0473,  0.0194,    -inf],\n",
      "          [-0.0046, -0.0365,  0.0474,  0.0868]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5324, 0.4676, 0.0000, 0.0000],\n",
      "          [0.3540, 0.2932, 0.3527, 0.0000],\n",
      "          [0.2625, 0.2406, 0.2563, 0.2406]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4851, 0.5149, 0.0000, 0.0000],\n",
      "          [0.3262, 0.3396, 0.3342, 0.0000],\n",
      "          [0.2427, 0.2563, 0.2447, 0.2563]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5240, 0.4760, 0.0000, 0.0000],\n",
      "          [0.4002, 0.3382, 0.2616, 0.0000],\n",
      "          [0.2901, 0.2528, 0.2032, 0.2540]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5070, 0.4930, 0.0000, 0.0000],\n",
      "          [0.3414, 0.3510, 0.3075, 0.0000],\n",
      "          [0.2524, 0.2464, 0.2370, 0.2641]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5089, 0.4911, 0.0000, 0.0000],\n",
      "          [0.3352, 0.3398, 0.3250, 0.0000],\n",
      "          [0.2435, 0.2424, 0.2329, 0.2811]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4953, 0.5047, 0.0000, 0.0000],\n",
      "          [0.3228, 0.3273, 0.3499, 0.0000],\n",
      "          [0.2429, 0.2352, 0.2558, 0.2661]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1964, -0.3370,  0.2351,  1.0265, -0.4812, -0.6810, -0.1689,\n",
       "            0.1994, -0.0489,  0.5304,  0.0134, -0.9497,  0.5102,  0.1043,\n",
       "            0.1859],\n",
       "          [-0.1075, -0.1018, -0.0634,  0.3674, -0.4453, -0.3625, -0.0774,\n",
       "           -0.2322,  0.2285,  0.1781, -0.0102, -0.3242,  0.3295, -0.1240,\n",
       "            0.0705],\n",
       "          [-0.1154, -0.0324, -0.1995,  0.5057, -0.4732, -0.4676, -0.1324,\n",
       "           -0.0560, -0.1153,  0.3052,  0.0340, -0.4541,  0.1761, -0.1603,\n",
       "            0.2385],\n",
       "          [-0.1442, -0.1273, -0.1234,  0.3718, -0.3597, -0.5065, -0.1249,\n",
       "           -0.0802, -0.0313,  0.2699, -0.0917, -0.3659,  0.3075, -0.0948,\n",
       "            0.0862]],\n",
       " \n",
       "         [[-0.0530, -0.3858,  0.1137,  0.3362, -0.3367, -0.6625, -0.0220,\n",
       "           -0.3477,  0.1733,  0.4738, -0.2000, -0.2620,  0.4429, -0.0338,\n",
       "           -0.0480],\n",
       "          [-0.0955, -0.3300,  0.1044,  0.3440, -0.2320, -0.6924, -0.0809,\n",
       "           -0.1743, -0.1724,  0.4130, -0.2077, -0.4203,  0.1250, -0.1314,\n",
       "            0.0125],\n",
       "          [-0.1822,  0.1017, -0.1327,  0.1547, -0.1166, -0.2465, -0.1405,\n",
       "           -0.1033, -0.0588, -0.0630, -0.1604, -0.1292, -0.1242, -0.2516,\n",
       "            0.1211],\n",
       "          [-0.1549, -0.2102, -0.0570,  0.3966, -0.3222, -0.5360, -0.0835,\n",
       "           -0.0337, -0.1489,  0.1253, -0.0605, -0.3301, -0.0043, -0.0202,\n",
       "            0.1885]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[[ 0.9034, -0.7540,  0.2806, -0.9340, -0.4092],\n",
       "           [ 0.5971, -0.7502, -0.1623, -0.2677, -0.4058],\n",
       "           [ 0.6417, -0.8555,  0.3945, -0.6136, -0.6273],\n",
       "           [ 0.4887, -0.2386, -0.1758, -0.4050, -0.4224]],\n",
       " \n",
       "          [[-0.1870,  0.0681,  0.0641, -0.4699, -0.3882],\n",
       "           [-0.1898,  0.1528,  0.2259, -0.1667, -0.4033],\n",
       "           [-0.0057,  0.0396,  0.3396, -0.3180, -0.2025],\n",
       "           [-0.1218,  0.3825,  0.2174, -0.1677, -0.3590]],\n",
       " \n",
       "          [[ 0.3613,  0.8869, -0.2899,  0.2541,  0.2689],\n",
       "           [-0.0426,  0.6725, -0.4009,  0.4592,  0.4336],\n",
       "           [ 0.1063,  0.2768, -0.6187,  0.5029,  0.1458],\n",
       "           [ 0.0409,  0.0784,  0.4147,  0.2188,  0.4419]]],\n",
       " \n",
       " \n",
       "         [[[ 0.8290, -0.6325,  0.3488, -0.6852, -0.5360],\n",
       "           [ 0.3361, -0.4662,  0.2627, -0.6663, -0.3454],\n",
       "           [ 0.5115, -0.7407,  0.0158, -0.4359, -0.3941],\n",
       "           [ 0.5940, -0.9471,  0.5488, -0.5952, -0.5102]],\n",
       " \n",
       "          [[ 0.0761,  0.1556,  0.1212, -0.2125, -0.1096],\n",
       "           [ 0.1539,  0.0198,  0.2354, -0.2488, -0.0543],\n",
       "           [ 0.0527,  0.0566,  0.0645, -0.2767, -0.2259],\n",
       "           [-0.2586,  0.2412,  0.0970, -0.1131,  0.0544]],\n",
       " \n",
       "          [[ 0.1382,  0.1722, -0.1282,  0.5288,  0.4938],\n",
       "           [ 0.0016, -0.1137, -0.0948,  0.2049,  0.4184],\n",
       "           [ 0.0660,  0.3758, -0.3376,  0.4549,  0.3362],\n",
       "           [ 0.4176,  0.5793, -0.3710,  0.5386,  0.1469]]]],\n",
       "        grad_fn=<PermuteBackward0>),\n",
       " tensor([[[[ 0.0388, -0.9394,  0.0849, -0.0657, -0.0215],\n",
       "           [ 0.0115, -0.5207, -0.1561, -0.0064, -0.0123],\n",
       "           [ 0.1935, -0.9925,  0.0234,  0.1503,  0.3780],\n",
       "           [ 0.0952, -0.5458,  0.5654,  0.2143,  0.0742]],\n",
       " \n",
       "          [[-0.1098, -0.1560,  0.1055,  0.9802,  0.9086],\n",
       "           [-0.0186, -0.1594,  0.2865,  0.6141,  0.2716],\n",
       "           [-0.0409, -0.2729,  0.5720,  0.6115,  0.5422],\n",
       "           [-0.3322,  0.0249,  0.2778,  0.2393,  0.2383]],\n",
       " \n",
       "          [[-0.2586, -0.0245, -0.1699,  0.3044,  0.3846],\n",
       "           [-0.2593, -0.0926,  0.1231,  0.1915,  0.3186],\n",
       "           [-0.4252, -0.1274,  0.1734,  0.2161, -0.1507],\n",
       "           [-0.2076,  0.0623, -0.0777,  0.7890,  0.2327]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2310, -0.9642,  0.2758,  0.0646,  0.1957],\n",
       "           [-0.0302, -0.9220,  0.2494,  0.2389,  0.2798],\n",
       "           [-0.0249, -0.5905,  0.1155, -0.1083, -0.0489],\n",
       "           [ 0.1766, -0.7838,  0.1468, -0.0990,  0.0049]],\n",
       " \n",
       "          [[-0.0203, -0.0860,  0.2977,  0.3184,  0.6492],\n",
       "           [-0.0383, -0.2434,  0.3025,  0.1017,  0.3927],\n",
       "           [-0.1021, -0.1557,  0.2776,  0.3852,  0.4141],\n",
       "           [-0.1411, -0.0872,  0.3961,  0.6918,  0.7633]],\n",
       " \n",
       "          [[-0.1726, -0.0621,  0.0671,  0.4357,  0.0984],\n",
       "           [-0.3019,  0.1071,  0.0296,  0.5156, -0.0717],\n",
       "           [ 0.0782,  0.0186,  0.0615,  0.2517,  0.1988],\n",
       "           [-0.1387, -0.3597,  0.0727,  0.2500,  0.5169]]]],\n",
       "        grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tril(torch.ones(4, 4))\n",
    "x = torch.rand(2, 4 ,15)\n",
    "a = MultiHeadAttention(15, 3)\n",
    "a(x,x,x, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model ,dropout):\n",
    "\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.ln = LayerNormalization()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        return self.ln(x1 + self.dropout(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,  dff, d_model, heads, dropout):\n",
    "\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.multi_attention = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(d_model, dropout) for _ in range(2)])\n",
    "\n",
    "        self.feed_forward = FeedForward(d_model, dff)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1, key, value = self.multi_attention(x, x, x)\n",
    "        x2 = self.residual_connections[0](x, x1)\n",
    "        x3 = self.feed_forward(x2)\n",
    "        x4 = self.residual_connections[1](x2, x3)\n",
    "        return x4\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dff, seq_length, d_model, heads,dropout, n = 6):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_layer = InputEmbedding(d_model, vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, seq_length, dropout)\n",
    "        \n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(dff, d_model, heads,dropout) for _ in range(n)])\n",
    "        print(type(self.encoder_blocks))\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.positional_encoding(self.embedding_layer(x))\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dff, d_model, heads, dropout):\n",
    "\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        # self.masked_attention = masked_attention\n",
    "        # self.residual_connections = residual_connections\n",
    "        # self.cross_attention = cross_attention\n",
    "        # self.feed_forward = feed_forward\n",
    "        self.masked_attention = MultiHeadAttention(d_model, heads)\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(d_model, dropout) for _ in range(3)])\n",
    "        self.cross_attention = MultiHeadAttention(d_model, heads)\n",
    "        self.feed_forward = FeedForward(d_model, dff)\n",
    "\n",
    "\n",
    "    def forward(self, x, x_enc, x_enc, mask):\n",
    "\n",
    "        x1= self.masked_attention(x, x, x, mask)\n",
    "        x2 = self.residual_connections[0](x, x1)\n",
    "\n",
    "        x3= self.cross_attention(x2, x_enc, x_enc)\n",
    "        x4 = self.residual_connections[1](x2, x3)\n",
    "\n",
    "        x5 = self.feed_forward(x4)\n",
    "        x6 = self.residual_connections[2](x4, x5)\n",
    "\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dff, seq_length, d_model, heads,dropout, n = 6):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = InputEmbedding(d_model, vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, seq_length, dropout)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(dff, d_model, heads,dropout) for _ in range(n)])\n",
    "\n",
    "        self.mask = m = torch.tril(torch.ones(seq_length, seq_length))\n",
    "\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, x_enc, x_enc):\n",
    "\n",
    "        x = self.positional_encoding(self.embedding_layer(x))\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, x_enc, x_enc, self.mask)\n",
    "        return self.softmax(self.linear(x))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size_source, vocab_size_target, seq_length_source, seq_length_taeget, d_model = 512, heads = 8, dropout = 0.1, dff = 2048):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(vocab_size_source, dff, seq_length_source, d_model, heads, dropout)\n",
    "        self.decoder = Decoder(vocab_size_target, dff, seq_length_target, d_model, heads, dropout)\n",
    "\n",
    "        for parameter in self.parameters():\n",
    "            nn.init.xavier_uniform_(parameter)\n",
    "\n",
    "    def forward(self, x_in, x_op):\n",
    "\n",
    "        x_enc= self.encoder(x_in)\n",
    "        output = self.decoder(x_op, x_enc, x_enc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20\n",
    "DFF = 2048\n",
    "SEQ_LENGTH = 3\n",
    "D_MODEL = 512\n",
    "HEADS = 2\n",
    "DROPOUT = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.ModuleList'>\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(VOCAB_SIZE, DFF, SEQ_LENGTH, D_MODEL, HEADS, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"check_uniform_bounds\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[259], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"check_uniform_bounds\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4, 5, 6, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer(x, x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[231], line 12\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x_in, x_op)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_in, x_op):\n\u001b[0;32m---> 12\u001b[0m     _, key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x_in)\n\u001b[1;32m     13\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x_op, key, value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[216], line 14\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_layer(x))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_blocks:\n\u001b[1;32m     17\u001b[0m         x, key, value \u001b[38;5;241m=\u001b[39m block(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[207], line 11\u001b[0m, in \u001b[0;36mInputEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(d_model)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = torch.rand((4, 3, 3))\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = nn.Softmax(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10//5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.view(4, 5, 2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
